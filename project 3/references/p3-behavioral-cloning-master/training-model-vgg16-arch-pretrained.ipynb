{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "target_shape = (80, 40) #original size 320, 160\n",
    "nb_cams = 3\n",
    "angle_shift = 0.1\n",
    "\n",
    "def load_csv(directory=''):\n",
    "    #data_dirs = ['dataset-1', 'dataset-2', 'dataset-3', 'dataset-4', 'dataset-5', 'dataset-water', 'data']\n",
    "    data_dirs = ['data'] \n",
    "\n",
    "    global csv_rows_train, csv_rows_val, csv_rows_test\n",
    "    csv_rows = []\n",
    "\n",
    "    for dr in data_dirs:\n",
    "        log_filename = dr + '/driving_log.csv'\n",
    "        img_path = dr + '/IMG/'\n",
    "\n",
    "        with open(log_filename, 'r') as csvfile:\n",
    "            logreader = csv.reader(csvfile)\n",
    "            if dr == 'data': #udacity data format\n",
    "                row = next(logreader)\n",
    "                print(row)\n",
    "            for row in logreader:\n",
    "                if dr == 'data': #udacity data format\n",
    "                    row[0] = dr + '/' + row[0].strip() #center\n",
    "                    row[1] = dr + '/' + row[1].strip() #left\n",
    "                    row[2] = dr + '/' + row[2].strip() #right\n",
    "                else: \n",
    "                    row[0] = img_path + row[0].split('\\\\')[-1] #center\n",
    "                    row[1] = img_path + row[1].split('\\\\')[-1] #left\n",
    "                    row[2] = img_path + row[2].split('\\\\')[-1] #right\n",
    "                csv_rows.append(row)\n",
    "    csv_rows = np.array(csv_rows)\n",
    "\n",
    "    #split the rows to train test val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    csv_rows_train, csv_rows_test = train_test_split(csv_rows, test_size=0.1)\n",
    "    csv_rows_train, csv_rows_val = train_test_split(csv_rows_train, test_size=0.1)\n",
    "   \n",
    "    print('CSV loaded, #train = ', len(csv_rows_train), \n",
    "                      '#val = ', len(csv_rows_val),\n",
    "                      '#test = ', len(csv_rows_test)) \n",
    "\n",
    "def load_image(img_filename, angle, images, angles):\n",
    "    img = cv2.imread(img_filename, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, target_shape)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                                        \n",
    "    images.append(img)\n",
    "    angles.append(float(angle))\n",
    "    \n",
    "\n",
    "def batch_generator(batch_size, source='train'):\n",
    "\n",
    "    if source == 'train':\n",
    "        csv_rows = csv_rows_train\n",
    "    elif source == 'val':\n",
    "        csv_rows = csv_rows_val\n",
    "    elif source == 'test':\n",
    "        csv_rows = csv_rows_test\n",
    "    else:\n",
    "        print('Error data segment unknown = ', source)\n",
    "\n",
    "    row_indices = range(csv_rows.shape[0])\n",
    "    \n",
    "    while 1:\n",
    "        chosen_indices = np.random.choice(row_indices, size=int(batch_size/nb_cams)) \n",
    "        #print(chosen_indices)\n",
    "        chosen_rows = csv_rows[chosen_indices]\n",
    "        #print(chosen_rows.shape)\n",
    "        images = []\n",
    "        angles = []\n",
    "        for row in chosen_rows:\n",
    "            load_image(row[0], float(row[3]), images, angles) #center\n",
    "            if nb_cams == 3:\n",
    "                load_image(row[1], float(row[3])+angle_shift, images, angles) #left\n",
    "                load_image(row[2], float(row[3])-angle_shift, images, angles) #right\n",
    "\n",
    "        preprocessed_imgs = preprocess_input(np.array(images).astype('float'))\n",
    "        X_train = preprocessed_imgs\n",
    "        y_train = np.array(angles)\n",
    "        #print('Batch generated.')\n",
    "        yield(X_train, y_train)\n",
    "\n",
    "# from https://github.com/Lasagne/Lasagne/issues/12\n",
    "def threaded_generator(generator, num_cached=10):\n",
    "    import queue\n",
    "    queue = queue.Queue(maxsize=num_cached)\n",
    "    sentinel = object()  # guaranteed unique reference\n",
    "\n",
    "    # define producer (putting items into queue)\n",
    "    def producer():\n",
    "        for item in generator:\n",
    "            queue.put(item)\n",
    "        queue.put(sentinel)\n",
    "\n",
    "    # start producer (in a background thread)\n",
    "    import threading\n",
    "    thread = threading.Thread(target=producer)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "    # run as consumer (read items from queue, in current thread)\n",
    "    item = queue.get()\n",
    "    while item is not sentinel:\n",
    "        yield item\n",
    "        queue.task_done()\n",
    "        item = queue.get()    \n",
    "\n",
    "def show_predictions(X_test, Y_test):\n",
    "    print('Predicting on test set...')\n",
    "    predictions = model.predict(X_test)\n",
    "    sum_p = 0\n",
    "    sum_y = 0\n",
    "    for i in range(32):\n",
    "        sum_p += predictions[i][0]\n",
    "        sum_y += Y_test[i]\n",
    "        print('p: %.4f' % predictions[i][0], ', y: %.4f' % Y_test[i],\n",
    "              'c_p: %.4f' % sum_p, ', c_y: %.4f' % sum_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, None, None, 64 0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, None, None, 64 0           block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, None, None, 64 0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, None, None, 12 0           block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, None, None, 12 0           block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 12 0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, None, None, 25 0           block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, None, None, 25 0           block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, None, None, 25 0           block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 25 0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, None, None, 51 0           block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, None, None, 51 0           block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, None, None, 51 0           block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 51 0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, None, None, 51 0           block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, None, None, 51 0           block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, None, None, 51 0           block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, None, None, 51 0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling2d_1 (Global (None, 512)           0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          525312      globalaveragepooling2d_1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1024)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           524800      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 256)           131328      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             257         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1181697\n",
      "____________________________________________________________________________________________________\n",
      "['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
      "CSV loaded, #train =  6508 #val =  724 #test =  804\n",
      "Epoch 1/100\n",
      "6426/6426 [==============================] - 28s - loss: 18.5913 - val_loss: 1.0573\n",
      "Epoch 2/100\n",
      "5166/6426 [=======================>......] - ETA: 3s - loss: 3.3365"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(1)(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "my_adam = Adam(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=my_adam)\n",
    "#model.compile(optimizer='rmsprop', loss='mean_absolute_error')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "load_csv()\n",
    "\n",
    "batch_size = nb_cams*int(128/nb_cams) # must be multiply of nb_cams\n",
    "samples_per_epoch = batch_size*int(len(csv_rows_train)/batch_size)\n",
    "nb_epoch = 100\n",
    "nb_val_samples = 5*batch_size \n",
    "\n",
    "history = model.fit_generator(threaded_generator(batch_generator(batch_size, 'train')),\n",
    "                    samples_per_epoch, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=batch_generator(batch_size, 'val'),\n",
    "                    nb_val_samples=nb_val_samples)\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "def visualize_history():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy\n",
    "\n",
    "    print(history.history.keys())\n",
    "    %matplotlib inline\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,np.max(history.history['val_loss'])])\n",
    "    plt.show()\n",
    "\n",
    "visualize_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_and_save():\n",
    "    test_samples = nb_cams*int(1000/nb_cams)\n",
    "    gen = threaded_generator(batch_generator(test_samples, 'test'))\n",
    "    X_test, Y_test = next(gen)\n",
    "    score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "    print('Test score (MSE): ', score)\n",
    "\n",
    "    filepath = 'vgg16-pretrained-gen-' + '%.4f' % score\n",
    "    # save model architecture:\n",
    "    json_string = model.to_json()\n",
    "    import json\n",
    "    with open(filepath + '.json', 'w') as outfile:\n",
    "        json.dump(json_string, outfile)\n",
    "    print('Model saved to ', filepath + '.json')\n",
    "    # save weights:\n",
    "    model.save_weights(filepath + '.h5')\n",
    "    print('Weights saved to ', filepath + '.h5')\n",
    "\n",
    "eval_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the whole model:\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "my_adam = Adam(lr=0.00001)\n",
    "model.compile(loss='mse', optimizer=my_adam)\n",
    "\n",
    "#nb_epoch = 10\n",
    "print('batch_size: ', batch_size)\n",
    "print('samples_per_epoch: ', samples_per_epoch)\n",
    "print('nb_epoch: ', nb_epoch)\n",
    "print('nb_val_samples: ', nb_val_samples)\n",
    "\n",
    "history = model.fit_generator(threaded_generator(batch_generator(batch_size, 'train')),\n",
    "                    samples_per_epoch, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=batch_generator(batch_size, 'val'),\n",
    "                    nb_val_samples=nb_val_samples)\n",
    "eval_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
